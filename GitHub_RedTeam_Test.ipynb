{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oebNWtQAoSp"
      },
      "source": [
        "**PyGithub Documentation Link:** https://pygithub.readthedocs.io/en/stable/index.html\n",
        "\n",
        "**PyGithub -> GitHub REST API Documentation:**  https://pygithub.readthedocs.io/en/stable/github.html\n",
        "\n",
        "**GitHub Search Query:** https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "oalpYOJo8q9s"
      },
      "outputs": [],
      "source": [
        "from github import Github, UnknownObjectException, Auth, GithubException\n",
        "from datetime import datetime, timezone\n",
        "from langchain_community.document_loaders import GithubFileLoader\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import configparser\n",
        "import tiktoken\n",
        "import openai\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "# GitHub API setup\n",
        "github_token = config['github']['token']\n",
        "auth = Auth.Token(github_token)\n",
        "g = Github(auth=auth)\n",
        "g = Github()\n",
        "\n",
        "# OpenAI Setup\n",
        "# Set the base URL and api_key for the RDSec One AI Endpoint API (Production)\n",
        "# Use the python-dotenv to load variables from env\n",
        "openai.base_url = config['openai']['api_endpoint']\n",
        "openai.api_key = config['openai']['api_key']\n",
        "openai.api_type = \"openai\"\n",
        "\n",
        "base_url = config['openai']['api_endpoint']\n",
        "api_key = config['openai']['api_key']\n",
        "model = \"gpt-4.1\"\n",
        "\n",
        "# VirusTotal setup\n",
        "vt_api_key = config['virustotal']['vt_api_key']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VjuCjF1yA9R",
        "outputId": "e7e2cfd0-9426-4b98-a36f-9d9c74bb982d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the script...\n",
            "Date today: 2025-06-11\n",
            "Checking connection to OpenAI\n",
            "Successfully connected to OpenAI\n",
            "Querying GitHub for repositories with query: repo:SaadAhla/dark-kill\n",
            "Removing Duplicates\n",
            "Total Repositories:1\n",
            "Processing repository: SaadAhla/dark-kill\n",
            "Data for repository SaadAhla/dark-kill has been written to json_output/Yes\\SaadAhla_dark-kill.json\n",
            "All repositories have been processed.\n",
            "The script finished successfully\n"
          ]
        }
      ],
      "source": [
        "def get_current_datetime_with_milliseconds():\n",
        "    # Get the current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Format the datetime object to include milliseconds\n",
        "    formatted_datetime = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    return formatted_datetime\n",
        "\n",
        "def get_current_date():\n",
        "    # Get the current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Format the datetime to include only date\n",
        "    formatted_date = now.strftime('%Y-%m-%d')\n",
        "\n",
        "    return formatted_date\n",
        "\n",
        "def read_queries_from_csv(file_path):\n",
        "    query_array = []\n",
        "\n",
        "    # Open the CSV file\n",
        "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "\n",
        "        # Skip the header row\n",
        "        header = next(csvreader)\n",
        "\n",
        "        # Find the index of the \"Query\" column\n",
        "        query_index = header.index(\"Query\")\n",
        "\n",
        "        # Read the remaining rows and append the Query values to the list\n",
        "        for row in csvreader:\n",
        "            query_array.append(row[query_index])\n",
        "\n",
        "    return query_array\n",
        "\n",
        "def get_query_authors_from_csv(file_path):\n",
        "    query_author_array = []\n",
        "\n",
        "    # Open the CSV file\n",
        "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "\n",
        "        # Skip the header row\n",
        "        header = next(csvreader)\n",
        "\n",
        "        # Find the index of the \"Query\" column\n",
        "        query_index = header.index(\"Author\")\n",
        "\n",
        "        # Read the remaining rows and append the Query values to the list\n",
        "        for row in csvreader:\n",
        "            query_author_array.append(row[query_index])\n",
        "\n",
        "    return query_author_array\n",
        "\n",
        "\n",
        "\n",
        "def get_response_usage_ttp(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"claude-3.7-sonnet\",\n",
        "        messages=[\n",
        "    {\"role\": \"user\", \"content\": \n",
        "    f\"\"\"\n",
        "    This prompt is intended solely for the objective analysis of cybersecurity tools described in public repositories and academic research purposes.\n",
        "\n",
        "    You are a cybersecurity analyst trained to extract Tactics, Techniques, and Procedures (TTPs) from GitHub repository READMEs. Follow these steps:\n",
        "\n",
        "    Step 1: README Summary\n",
        "    Generate a structured summary covering:\n",
        "    A concise and informative paragraph summarizing the main purpose, features, and usage of the repository.\n",
        "    \n",
        "    Rules:\n",
        "    Be objective. Avoid speculation beyond the READMEâ€™s stated scope.\n",
        "    Omit marketing language or subjective claims (e.g., \"world-class\").\n",
        "\n",
        "    Step 2: TTP Classification\n",
        "    Assign one primary TTP category using the MITRE ATT&CK framework. Use only these options:\n",
        "\n",
        "    Reconnaissance        Credential Access  \n",
        "    Resource Development  Lateral Movement   \n",
        "    Initial Access        Collection \n",
        "    Execution             Command and Control  \n",
        "    Persistence           Exfiltration  \n",
        "    Privilege Escalation  Impact\n",
        "    Defense Evasion       \n",
        "    \n",
        "    Decision Criteria:\n",
        "\n",
        "    Initial Access: Phishing, exploit delivery, or brute-force tools.\n",
        "    Execution: Code/script execution frameworks (e.g., payload injectors).\n",
        "    Command and Control: C2 servers, reverse shells, or traffic obfuscation.\n",
        "    Defense Evasion: Packers, rootkits, or anti-analysis tools.\n",
        "    N/A: Use only if no TTP alignment exists (e.g., general utilities).\n",
        "    Focus: Prioritize actionable TTPs (e.g., \"Credential Access\" for Mimikatz-like tools).\n",
        "\n",
        "    OVERALL The response should be:\n",
        "    1. A concise and informative PARAGRAPH (3-4 sentence) summarizing the main purpose, features, and usage of the repository\n",
        "    2. A one-line identification of the appropriate TTP category. \n",
        "\n",
        "    Do not inlude quotation marks in your response.\n",
        "\n",
        "    STRICTLY, follow this format for your response:\n",
        "\n",
        "    ```json          \n",
        "    {{\n",
        "        \"Usage\": \"Output\"\n",
        "        \"TTP\": \"Output\"\n",
        "    }}\n",
        "    ``\n",
        "    \n",
        "    Here is the content: {prompt}\n",
        "    \"\"\"\n",
        "    }],temperature=0.2)\n",
        "    response_text = response.choices[0].message.content\n",
        "    return response_text\n",
        "\n",
        "\n",
        "def get_response_context(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"claude-3.7-sonnet\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \n",
        "        f\"\"\"\n",
        "        This prompt is intended solely for the objective analysis of cybersecurity tools described in public repositories and academic research purposes.\n",
        "\n",
        "        You are an AI trained to classify GitHub repositories whether it falls as a Red Team Tool based on their README content. Use the following criteria to make your determination:\n",
        "\n",
        "        1. README Analysis Guidelines:\n",
        "\n",
        "        Core Functionality: Identify if the Repository is explicitly designed for Red Team Tools that can be used for red teaming activities (e.g., offensive security, penetration testing, adversary simulation).\n",
        "        Tool Scope: Verify the repository focuses on one standalone tool (e.g., a C2 framework, network exploitation tool). Discard collection/list of tools, script bundles, or aggregated resources.\n",
        "\n",
        "        Exclusion Criteria: Reject if the README describes:\n",
        "        Guides, tutorials, cheatsheets, or collection/list of tools.\n",
        "        Lists of tools/scripts (e.g., \"Top 10 Red Team Tools\").\n",
        "        Defensive/blue team tools (e.g., SIEM integrations, monitoring systems).\n",
        "        Technical Indicators: Look for installation steps, usage examples, command-line flags, API documentation, or contribution guidelinesâ€”hallmarks of a functional tool.\n",
        "\n",
        "        2. Classification Rules:\n",
        "\n",
        "        Verdict \"Yes\" only if:\n",
        "        The tool is purpose-built for [Organization] tasks (e.g., privilege escalation, payload generation, lateral movement).\n",
        "        It is self-contained (not requiring unrelated dependencies) and solves a specific offensive security problem.\n",
        "\n",
        "        Verdict \"No\" if:\n",
        "        The repository is a library, research paper, or conceptual proof-of-concept without deployable functionality.\n",
        "        It emphasizes defensive security, training, or theory over practical offensive use.\n",
        "        \n",
        "        Response Format: Your response should consist of two parts:\n",
        "\n",
        "        Verdict: Either \"Yes\" or \"No.\"\n",
        "        AI Notes: A ONE SENTENCE explanation of your reasoning behind the verdict, detailing the key factors that influenced your classification.\n",
        "\n",
        "        Do not inlude quotation marks in your response.\n",
        "\n",
        "        Strictly follow this format for your response:\n",
        "        ```json\n",
        "        {{\n",
        "            \"Verdict\": \"Output (Yes or No)\",\n",
        "            \"AI Notes\": \"Output\"\n",
        "        }}\n",
        "        ```\n",
        "             \n",
        "        Here is the content:{prompt}\n",
        "            \"\"\"\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "    response_text = response.choices[0].message.content\n",
        "    return response_text\n",
        "\n",
        "\n",
        "def extract_data(response, key):\n",
        "    # Clean the response\n",
        "    clean_response = response.replace(\"```json\",\"\").replace(\"```\",\"\")\n",
        "    parsed_json = (f'''{clean_response}''')\n",
        "    # Parse the cleaned JSON response\n",
        "    data = json.loads(parsed_json)\n",
        "    \n",
        "    # Return the requested data\n",
        "    return data.get(key)\n",
        "\n",
        "\n",
        "def remove_think(response):\n",
        "    cleaned_string = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
        "    return cleaned_string\n",
        "\n",
        "\n",
        "\n",
        "def get_total_token(prompt):\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "    input_tokens=encoding.encode(prompt)\n",
        "    return len(input_tokens)\n",
        "\n",
        "def check_assets_for_exe(repo_name):\n",
        "    try:\n",
        "        asset_container = []\n",
        "        repo = g.get_repo(repo_name)\n",
        "        latest_release = repo.get_latest_release()\n",
        "\n",
        "        for asset in latest_release.get_assets():\n",
        "            asset_container.append(asset)\n",
        "\n",
        "        # Check if asset_container has any data\n",
        "        if asset_container:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return False\n",
        "    \n",
        "\n",
        "def string_split_add_exe(repo_name):\n",
        "    part_after_slash = repo_name.split('/')[1]\n",
        "    append_exe = part_after_slash + \".exe\"\n",
        "    return append_exe\n",
        "\n",
        "\n",
        "def find_exe_files(repo_name, path=\"\"):\n",
        "    get_name_repo = repo_name.full_name\n",
        "    repo_exe_to_check = string_split_add_exe(get_name_repo)\n",
        "    exe_files = []\n",
        "    contents = repo_name.get_contents(path)\n",
        "    for content in contents:\n",
        "        if content.type == \"dir\":\n",
        "            # Recursively search in the subdirectory\n",
        "            if find_exe_files(repo_name, content.path):\n",
        "                return True\n",
        "        elif content.type == \"file\" and content.path.endswith(\".exe\"):\n",
        "            exe_files.append(content.path.lower())\n",
        "\n",
        "    repo_exe_to_check_lower = repo_exe_to_check.lower()\n",
        "\n",
        "    # Check if the filename of the repo appended with exe exists in the scraped exe files in the content\n",
        "    if repo_exe_to_check_lower in exe_files:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "'''\n",
        "def summarize_maxim_token(readme_doc):\n",
        "    llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=model)\n",
        "    char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=0)\n",
        "    readme_content = char_text_splitter.split_documents(readme_doc)\n",
        "    chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
        "    initial_contents = chain.invoke(readme_content[0:10])\n",
        "    return initial_contents.get('output_text','')  \n",
        "'''\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def populate_repo_data(repo_name, query, query_author):\n",
        "    #col_names = [\"Date\", \"Repository Name\", \"Stars\", \"Creation Date\", \"URL\", \"TTP\", \"Usage\",\"Query\", \"Query Author\"]\n",
        "    has_ioc = \"\"\n",
        "    init_repo = g.get_repo(repo_name)\n",
        "    repo_fullname = init_repo.full_name\n",
        "    # Populate List\n",
        "    #date_time = get_current_datetime_with_milliseconds()\n",
        "    #repo_obj = g.get_repo(repo)\n",
        "    #print(repo_obj)\n",
        "    #Check for exe in repo dir and assets\n",
        "    #has_exe_file_repo = find_exe_files(init_repo)\n",
        "    has_releases_assets = check_assets_for_exe(repo_name)\n",
        "    if has_releases_assets:\n",
        "        has_ioc = \"Yes\"\n",
        "    else:\n",
        "        has_ioc = \"No\"\n",
        "    \n",
        "    decoded_content = get_readme_contents(repo_name)\n",
        "    repo_formatted_date = init_repo.created_at.strftime('%Y-%m-%d')\n",
        "    repo_star = init_repo.stargazers_count\n",
        "    repo_url = f\"https://www.github.com/{repo_fullname}\"\n",
        "    repo_query = query\n",
        "    repo_query_author = query_author\n",
        "    #last_commit_date = get_latest_commit_date(repo_fullname)\n",
        "    exe_name = string_split_add_exe(repo_fullname)\n",
        "    #is_available_in_vt =  query_reponame_in_vt(exe_name)\n",
        "\n",
        "    prompt_ini = decoded_content\n",
        "    #Calculate Token from Prompt\n",
        "    input_token_total = get_total_token(prompt_ini)\n",
        "    #input_usage_token_total = get_total_token(prompt_for_usage)\n",
        "    #print(input_usage_token_total)\n",
        "\n",
        "    if input_token_total >= 128000:\n",
        "        return \"README exceeded the token limit\"\n",
        "    else:\n",
        "        #repo_ttp_ai = get_response(prompt_for_ttp)\n",
        "        #repo_usage_ai = get_response(prompt_for_usage)\n",
        "        response_ttp_usage = get_response_usage_ttp(prompt_ini)\n",
        "        response_context = get_response_context(prompt_ini)\n",
        "        removed_think_response = remove_think(response_context)\n",
        "        #print (removed_think_response)\n",
        "\n",
        "        extracted_ttp = extract_data(response_ttp_usage, \"TTP\")\n",
        "        extracted_usage = extract_data(response_ttp_usage,\"Usage\")\n",
        "        extracted_verdict = extract_data(removed_think_response, \"Verdict\")\n",
        "        extracted_notes = extract_data(removed_think_response, \"AI Notes\")\n",
        "        \n",
        "        repo_data = {\n",
        "        \"Repository Name\": repo_fullname,\n",
        "        \"Creation Date\": repo_formatted_date,\n",
        "        \"Verdict\": extracted_verdict,\n",
        "        \"AI Notes\": extracted_notes,\n",
        "        \"Stars\": repo_star,\n",
        "        \"URL\": repo_url,\n",
        "        \"TTP\": extracted_ttp,\n",
        "        \"Usage\": extracted_usage,\n",
        "        \"Has IOC\": has_ioc,\n",
        "        #\"Is VT available\": is_available_in_vt,\n",
        "        \"Query\": repo_query,\n",
        "        \"Query Author\": repo_query_author\n",
        "        }\n",
        "\n",
        "        \n",
        "    return repo_data\n",
        "\n",
        "def get_readme_contents(repo_name):\n",
        "    # Regex pattern to match files containing \"README\" in their filename, case-insensitive\n",
        "    readme_pattern = re.compile(r'readme\\.(txt|md|rst|org)$', re.IGNORECASE)\n",
        "\n",
        "    # Initialize GitHub API client\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    # Get the default branch\n",
        "    default_branch = repo.default_branch\n",
        "\n",
        "    try:\n",
        "        loader = GithubFileLoader(\n",
        "            repo=repo_name,  # the repo name\n",
        "            branch=default_branch,  # the defaultzqy1 branch name\n",
        "            access_token=github_token,\n",
        "            github_api_url=\"https://api.github.com\",\n",
        "            file_filter=lambda file_path: readme_pattern.search(file_path) is not None\n",
        "        )\n",
        "        readme_content = loader.load()\n",
        "        if not readme_content:\n",
        "            return \"No README files found in the repository.\"\n",
        "        return readme_content[0].page_content\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Unable to Load README files: {e}\")\n",
        "            \n",
        "\n",
        "def load_existing_repos(json_file_path):\n",
        "    existing_repos = []\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            existing_repos.extend(data)\n",
        "    except FileNotFoundError:\n",
        "        pass  # If the file does not exist, we just skip it\n",
        "    return existing_repos\n",
        "\n",
        "def load_existing_repo_names(*json_file_path):\n",
        "    existing_repo_names = set()\n",
        "    for json_file_path in json_file_path:\n",
        "        try:\n",
        "            with open(json_file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                for item in data:\n",
        "                    existing_repo_names.add(item[\"Repository Name\"].split('/')[-1])\n",
        "        except FileNotFoundError:\n",
        "            pass  # If the file does not exist, we just skip it\n",
        "    return existing_repo_names\n",
        "\n",
        "def append_data_to_json_with_date(json_file_path, repo_data):\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(json_file_path):\n",
        "        # If it exists, read the existing data\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    # Append new data to existing data\n",
        "    existing_data.extend(repo_data)\n",
        "\n",
        "    # Write back to the JSON file\n",
        "    with open(json_file_path, 'w') as f:\n",
        "        json.dump(existing_data, f, indent=4)\n",
        "\n",
        "def append_data_to_json_archive(json_file_path, repo_data):\n",
        "   # Check if the file exists\n",
        "    if os.path.exists(json_file_path):\n",
        "        # If it exists, read the existing data\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    # Append new data to existing data\n",
        "    existing_data.extend(repo_data)\n",
        "\n",
        "    # Write back to the JSON file\n",
        "    with open(json_file_path, 'w') as f:\n",
        "        json.dump(existing_data, f, indent=4)\n",
        "\n",
        "\n",
        "def is_api_key_valid():\n",
        "    prompt = \"This is a test\"\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    )\n",
        "    except Exception as e:\n",
        "        print(f\"An error occured: {e}\")\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def query_reponame_in_vt(repo_fullname):\n",
        "    query = (f'name:{repo_fullname}')\n",
        "    url = 'https://www.virustotal.com/api/v3/intelligence/search'\n",
        "    headers = {\n",
        "        'x-apikey': vt_api_key\n",
        "    }\n",
        "    params = {\n",
        "        'query': query\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        " \n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "        if results.get('data'):\n",
        "            return f\"https://www.virustotal.com/gui/search/name%253A{repo_fullname}/files\"\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}, {response.text}\"\n",
        "    \n",
        "\n",
        "def save_updated_repos(json_file_path, data):\n",
        "    with open(json_file_path, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "def get_query_and_author_by_repo_name(json_data, repo_name):\n",
        "    for repo in json_data:\n",
        "        if repo[\"Repository Name\"] == repo_name:\n",
        "            return repo[\"Query\"], repo[\"Query Author\"]\n",
        "    return None, None\n",
        "\n",
        "def check_rate_limit():\n",
        "    rate_limit = g.get_rate_limit()\n",
        "    core_rate = rate_limit.core\n",
        "    search_rate = rate_limit.search\n",
        "    \n",
        "    if core_rate.remaining < 10 or search_rate.remaining < 5:\n",
        "        reset_time = max(core_rate.reset, search_rate.reset)\n",
        "        # Use timezone-aware current time\n",
        "        current_time = datetime.now(timezone.utc)\n",
        "        sleep_time = (reset_time - current_time).total_seconds()\n",
        "        if sleep_time > 0:\n",
        "            print(f\"Rate limit nearly exhausted. Sleeping for {sleep_time} seconds.\")\n",
        "            time.sleep(sleep_time + 5)  # Add 5 seconds buffer\n",
        "def main():\n",
        "    print(\"Starting the script...\")\n",
        "\n",
        "# Get Date\n",
        "    date_today = get_current_date()\n",
        "    print(\"Date today: \" + date_today)\n",
        "\n",
        "# Initialization of Variables and Queries\n",
        "    \n",
        "    query_array = read_queries_from_csv(\"github_queries.csv\")\n",
        "    query_author_array = get_query_authors_from_csv(\"github_queries.csv\")\n",
        "    json_file_path_based_on_date = \"github_repositories_\"+ date_today +\".json\"\n",
        "    json_file_path_archive = \"github_repositories_archive.json\"\n",
        "    final_repo_name_list = []\n",
        "    new_repositories = []\n",
        "    output_directory_yes = 'json_output/Yes'\n",
        "    output_directory_no = 'json_output/No'\n",
        "\n",
        "# Check OpenAI API Connection\n",
        "    print(\"Checking connection to OpenAI\")\n",
        "    openai_status = is_api_key_valid()\n",
        "    if openai_status:\n",
        "        print(\"Successfully connected to OpenAI\")\n",
        "    else:\n",
        "        print(\"Cannot connect to OpenAI. The Script is exiting....\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Load Existing Repositories from Archive CSV   \n",
        "    existing_repo_names = load_existing_repo_names(json_file_path_archive)\n",
        "    \n",
        "\n",
        "    for query in query_array:\n",
        "        \n",
        "        print(f\"Querying GitHub for repositories with query: {query}\")\n",
        "        repos = g.search_repositories(query=query, stars='<1000')\n",
        "        print(\"Removing Duplicates\")\n",
        "        query_repo_count = 0\n",
        "        for repo in repos:\n",
        "            if repo.full_name.split('/')[-1] not in existing_repo_names and query_repo_count < 10 :\n",
        "                new_repositories.append({\n",
        "                    \"Repository Name\": repo.full_name,\n",
        "                    \"Query\": query,\n",
        "                    \"Query Author\": query_author_array[query_array.index(query)]\n",
        "                })\n",
        "            \n",
        "                #new_repositories.append(repo.full_name)\n",
        "                existing_repo_names.add(repo.full_name)\n",
        "                query_repo_count += 1\n",
        "                time.sleep(5)\n",
        "\n",
        "    print(f\"Total Repositories:{len(new_repositories)}\")\n",
        "    for new_repo in new_repositories:\n",
        "            repo_name = new_repo['Repository Name']\n",
        "            query = new_repo['Query']\n",
        "            query_author = new_repo['Query Author']\n",
        "            if new_repo is None:\n",
        "                continue\n",
        "            try:\n",
        "                print(f\"Processing repository: {repo_name}\")\n",
        "                check_rate_limit()\n",
        "                repo_data = populate_repo_data(repo_name, query, query_author)\n",
        "                final_repo_name_list.append({\n",
        "                \"Repository Name\": repo_name,\n",
        "                \"Query\": query,\n",
        "                \"Query Author\": query_author\n",
        "                })\n",
        "                try:\n",
        "                    append_data_to_json_archive(json_file_path_archive, final_repo_name_list)\n",
        "                except Exception as e:\n",
        "                    print(f\"An error occured while appending data to JSON file: {e}\")\n",
        "                final_repo_name_list = []\n",
        "            \n",
        "                if repo_data == \"README exceeded the token limit\":\n",
        "                    print(\"Repository exceeded the token limit. Skipping...\")\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "\n",
        "                else:\n",
        "                    if repo_data.get(\"Verdict\",\"\") == \"Yes\":\n",
        "                        # Define the JSON file name based on the repository name\n",
        "                        json_file_name = os.path.join(output_directory_yes, f\"{repo_name.replace('/', '_')}.json\")\n",
        "\n",
        "                        # Write the repository data to a JSON file\n",
        "                        with open(json_file_name, 'w') as json_file:\n",
        "                            json.dump(repo_data, json_file, indent=4)\n",
        "                        print(f\"Data for repository {repo_name} has been written to {json_file_name}\")\n",
        "                        time.sleep(5)\n",
        "                    else:\n",
        "                         # Define the JSON file name based on the repository name\n",
        "                        json_file_name = os.path.join(output_directory_no, f\"{repo_name.replace('/', '_')}.json\")\n",
        "\n",
        "                        # Write the repository data to a JSON file\n",
        "                        with open(json_file_name, 'w') as json_file:\n",
        "                            json.dump(repo_data, json_file, indent=4)\n",
        "                        print(f\"Data for repository {repo_name} has been written to {json_file_name}\")\n",
        "                        time.sleep(3)\n",
        "            except Exception as e:\n",
        "                #print(repo_data)\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "    print(\"All repositories have been processed.\")\n",
        "\n",
        "    #print(f\"Appending repositories to JSON file: {json_file_path_archive}\")\n",
        "    #Append existing Repository to github_repositories_archive.json\n",
        "    try:\n",
        "        append_data_to_json_archive(json_file_path_archive, final_repo_name_list)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occured while appending data to JSON file: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    '''\n",
        "     for repo_data in final_repo_data_list:\n",
        "        try:\n",
        "            append_data_to_json_with_date(json_file_path_based_on_date, repo_data)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occured while appending data to JSON file: {e}\")\n",
        "            sys.exit(1)\n",
        "    '''\n",
        "\n",
        "    print(\"The script finished successfully\")\n",
        "    g.close()\n",
        "    \n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FOR TESTING PURPOSES ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TEST CONTENT PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'configparser' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\cjarsleym\\Desktop\\red-team_automation_files\\GitHub_RedTeam_Test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cjarsleym/Desktop/red-team_automation_files/GitHub_RedTeam_Test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m config \u001b[39m=\u001b[39m configparser\u001b[39m.\u001b[39mConfigParser()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cjarsleym/Desktop/red-team_automation_files/GitHub_RedTeam_Test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m config\u001b[39m.\u001b[39mread(\u001b[39m'\u001b[39m\u001b[39mconfig.ini\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cjarsleym/Desktop/red-team_automation_files/GitHub_RedTeam_Test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# GitHub API setup\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'configparser' is not defined"
          ]
        }
      ],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "# GitHub API setup\n",
        "github_token = config['github']['token']\n",
        "auth = Auth.Token(github_token)\n",
        "g = Github(auth=auth)\n",
        "\n",
        "loader = GithubFileLoader(\n",
        "    repo=\"gussieIsASuccessfulWarlock/Cyber-Logic-Dataset\",  # the repo name\n",
        "    branch=\"main\",  # the branch name\n",
        "    access_token=github_token,\n",
        "    github_api_url=\"https://api.github.com\",\n",
        "    file_filter=lambda file_path: file_path.endswith(\n",
        "        \".md\"\n",
        "    ),  # load all markdowns files.\n",
        ")\n",
        "documents = loader.load()\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README not found\n"
          ]
        }
      ],
      "source": [
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "# GitHub API setup\n",
        "github_token = config['github']['token']\n",
        "auth = Auth.Token(github_token)\n",
        "g = Github(auth=auth)\n",
        "\n",
        "\n",
        "\n",
        "def get_readme_contents(repo_name):\n",
        "    common_dirs = [\n",
        "        '',  # root directory\n",
        "        '.github'\n",
        "    ]\n",
        "    \n",
        "    # Regex pattern to match files containing \"README\" in their filename, case-insensitive\n",
        "    readme_pattern = re.compile(r'readme\\.(txt|md|rst|org|adoc|asciidoc|html|pod|creole|mediawiki)$', re.IGNORECASE)\n",
        "    \n",
        "    # Initialize Github API client\n",
        "    repo = g.get_repo(repo_name)\n",
        "    \n",
        "    for directory in common_dirs:\n",
        "        try:\n",
        "            contents = repo.get_contents(directory)\n",
        "            for content_file in contents:\n",
        "                if readme_pattern.match(content_file.name):\n",
        "                    readme_content = repo.get_contents(content_file.path)\n",
        "                    if readme_content.encoding == 'none':\n",
        "                        return \"README not found\"\n",
        "                    else:\n",
        "                        return readme_content.decoded_content.decode()\n",
        "        except UnknownObjectException:\n",
        "            continue\n",
        "        except GithubException as e:\n",
        "            print(f\"Error accessing directory '{directory}': {e}\")\n",
        "            continue\n",
        "\n",
        "    return \"README not found\"\n",
        "print(get_readme_contents(\"gussieIsASuccessfulWarlock/Cyber-Logic-Dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Certainly! Could you please provide the text you would like summarized?\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import GithubFileLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "base_url = config['openai']['api_endpoint']\n",
        "api_key = config['openai']['api_key']\n",
        "model = \"gpt-4o\"\n",
        "\n",
        "# GitHub API setup\n",
        "github_token = config['github']['token']\n",
        "auth = Auth.Token(github_token)\n",
        "g = Github(auth=auth)\n",
        "\n",
        "def get_readme_contents(repo_name):\n",
        "    # Regex pattern to match files containing \"README\" in their filename, case-insensitive\n",
        "    readme_pattern = re.compile(r'readme\\.(txt|md|rst|org|adoc|asciidoc|html|pod|creole|mediawiki)$', re.IGNORECASE)\n",
        "    # Initialize GitHub API client\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    # Get the default branch\n",
        "    default_branch = repo.default_branch\n",
        "\n",
        "    try:\n",
        "        loader = GithubFileLoader(\n",
        "            repo=repo_name,  # the repo name\n",
        "            branch=default_branch,  # the defaultzqy1 branch name\n",
        "            access_token=github_token,\n",
        "            github_api_url=\"https://api.github.com\",\n",
        "            file_filter=lambda file_path: readme_pattern.search(file_path) is not None or file_path == \"README.md\"\n",
        "        )\n",
        "        readme_content = loader.load()\n",
        "        print(readme_content)\n",
        "        return readme_content\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading README files: {e}\")\n",
        "    \n",
        "    raise ValueError(\"No README files found in the default branch of the repository.\")\n",
        "\n",
        "\n",
        "\n",
        "readme = get_readme_contents(\"gussieIsASuccessfulWarlock/Cyber-Logic-Dataset\")\n",
        "\n",
        "llm = ChatOpenAI(api_key=api_key, base_url=base_url, model=model)\n",
        "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "readme_content = char_text_splitter.split_documents(readme)\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
        "initial_contents = chain.invoke(readme_content[0:10])\n",
        "print(initial_contents.get('output_text',''))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from github import Github\n",
        "from langchain_community.document_loaders import GithubFileLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "# GitHub API setup\n",
        "github_token = config['github']['token']\n",
        "auth = Auth.Token(github_token)\n",
        "g = Github(auth=auth)\n",
        "\n",
        "def get_readme_contents(repo_name):\n",
        "    # Regex pattern to match files containing \"README\" in their filename, case-insensitive\n",
        "    readme_pattern = re.compile(r'readme\\.(txt|md|rst)$', re.IGNORECASE)\n",
        "\n",
        "    # Initialize GitHub API client\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    # Get the default branch\n",
        "    default_branch = repo.default_branch\n",
        "\n",
        "    try:\n",
        "        loader = GithubFileLoader(\n",
        "            repo=repo_name,  # the repo name\n",
        "            branch=default_branch,  # the defaultzqy1 branch name\n",
        "            access_token=github_token,\n",
        "            github_api_url=\"https://api.github.com\",\n",
        "            file_filter=lambda file_path: readme_pattern.search(file_path) is not None\n",
        "        )\n",
        "        readme_content = loader.load()\n",
        "        if readme_content:\n",
        "            return readme_content[0].page_content\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading README files: {e}\")\n",
        "\n",
        "    raise ValueError(\"No README files found in the default branch of the repository.\")\n",
        "\n",
        "'''\n",
        "import json\n",
        "\n",
        "# Specify the path to the JSON file\n",
        "file_path = 'github_repositories_archive_2025.json'\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open(file_path, 'r') as file:\n",
        "    repositories = json.load(file)\n",
        "\n",
        "# Loop through each repository and print the repository name\n",
        "'''\n",
        "\n",
        "\n",
        "def get_response_context(prompt):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \n",
        "            \"\"\"\n",
        "        Simulate three brilliant, logical experts in penetration testing collaboratively answering a question. Each one verbosely explains their thought process in real-time, \n",
        "        onsidering the prior explanations of others and openly acknowledging mistakes. At each step, whenever possible, each expert refines and builds \n",
        "        upon the thoughts of others, acknowledging their contributions. They continue until there is a definitive answer to the question. \n",
        "        The idea is that they are a professional pentester and will verdict if the repository given is a tool that they can use to in their pentesting methods.\n",
        "\n",
        "\n",
        "        You are responsible for determining whether a given GitHub repository qualifies as a Red Team Tool or is merely a collection of guides, cheatsheets, tips, and collections of tools. Follow these guidelines:\n",
        "\n",
        "        README Analysis: Carefully read and analyze the README file of the GitHub repository to gather insights about its purpose and functionality.\n",
        "        To verify if it is a tool, think of it if it can be classified as one of MITRE TTPS when used or can be used as a tool to use to pentest a system.\n",
        "\n",
        "        Classification Criteria:\n",
        "\n",
        "        If the repository is specifically designed as a Red Team Tool, respond with \"Yes.\"\n",
        "        If the repository is not a Red Team Tool or is primarily a guide, cheatsheet, or collection, respond with \"No.\"\n",
        "        Also add a confidence level whether you think the repository is a tool (100 being the high confidence while 0 being the low)\n",
        "        Response Format: Your response should consist of three parts:\n",
        "\n",
        "        The verdict: either \"Yes\" or \"No.\"\n",
        "        The AI notes: A brief explanation of your reasoning behind the verdict, detailing the key factors that influenced your classification.\n",
        "        The Confidence Level: A percentage whether you think the repository is a tool that can be use by pentesters\n",
        "        \n",
        "        Strictly, your output should be in a JSON Format like this:          \n",
        "            {\n",
        "                \"Verdict\": \"Output\",\n",
        "                \"AI Notes\": \"Output\"\n",
        "                \"Confidence Level\": \"Output\"\n",
        "            }\n",
        "\n",
        "        Example Output:\n",
        "            {\n",
        "                \"Verdict\": No,\n",
        "                \"AI Notes\": The repository 'js-cookie-monitor-debugger-hook' primarily serves as a script for monitoring and debugging JavaScript operations related to cookies. It provides functionalities to track changes to cookies and set breakpoints when specific cookie events occur. While it is a useful tool for developers and security researchers, it does not fit the criteria of a Red Team Tool as defined by MITRE TTPs. Instead, it functions more as a debugging aid rather than a tool designed for offensive security operations.\n",
        "                \"Confidence Level\": 0% \n",
        "            }\n",
        "\n",
        "            \"\"\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.1\n",
        "    )\n",
        "    response_text = response.choices[0].message.content\n",
        "    return response_text\n",
        "\n",
        "'''\n",
        "\n",
        "decoded_content = get_readme_contents(init_repo) \n",
        "prompt = f\"Here is the link: {decoded_content}\"\n",
        "\n",
        "print(get_response(prompt))\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "for repo in repositories:\n",
        "    link = f\"https://github.com/{repo['Repository Name']}\"\n",
        "    prompt = f\"Here is the link: {link}\"\n",
        "    response = get_response(prompt)\n",
        "    print(f\"Repository Name: {repo['Repository Name']}, Response: {response}\")\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Assuming the CSV file is named 'repositories.csv' and contains one URL per line\n",
        "file_path = 'test_repo.csv'\n",
        "\n",
        "# Open the CSV file and read the URLs\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read each line and strip any whitespace (like newline characters)\n",
        "    repositories = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Loop through each URL and perform actions\n",
        "for repo in repositories:\n",
        "    # Print the URL (or perform any other desired action)\n",
        "    readme_content = get_readme_contents(repo)\n",
        "    prompt = f\"Here is the content: {readme_content}\"\n",
        "    response = get_response_context(prompt)\n",
        "    print(f\"Repository Name: {repo}, Response: {response}\")\n",
        "    # You can add more actions here, such as calling a function\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test recursive search in repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "def check_assets_for_exe(repo_name):\n",
        "    try:\n",
        "        asset_container = []\n",
        "        repo = g.get_repo(repo_name)\n",
        "        latest_release = repo.get_latest_release()\n",
        "\n",
        "        for asset in latest_release.get_assets():\n",
        "            asset_container.append(asset)\n",
        "\n",
        "        # Check if asset_container has any data\n",
        "        if asset_container:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "status = check_assets_for_exe(\"jekil/awesome-hacking\")\n",
        "\n",
        "print (status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test commit date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[]]\n"
          ]
        }
      ],
      "source": [
        "json_file_path_archive = \"github_repositories_archive.json\"\n",
        "new_repositories = []\n",
        "\n",
        "\n",
        "def get_current_date():\n",
        "    # Get the current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Format the datetime to include only date\n",
        "    formatted_date = now.strftime('%Y-%m-%d')\n",
        "\n",
        "    return formatted_date\n",
        "\n",
        "def load_existing_repos(json_file_path):\n",
        "    existing_repos = []\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            existing_repos.extend(data)\n",
        "    except FileNotFoundError:\n",
        "        pass  # If the file does not exist, we just skip it\n",
        "    return existing_repos\n",
        "\n",
        "def get_repositories_with_updated_commit(json_file_path_archive):\n",
        "    existing_repos = load_existing_repos(json_file_path_archive)\n",
        "    date_today = get_current_date()\n",
        "    updated_repos = []\n",
        "\n",
        "    for repo in existing_repos:\n",
        "        # Compare the \"Last Commit Date\" to the current date\n",
        "        if repo[\"Last Commit Date\"] == date_today:\n",
        "            # Append the repository to the list if the dates match\n",
        "            updated_repos.append(repo[\"Repository Name\"])\n",
        "    \n",
        "    return updated_repos\n",
        "\n",
        "matching_repos = get_repositories_with_updated_commit(json_file_path_archive)\n",
        "new_repositories.append(matching_repos)\n",
        "print(new_repositories)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test content fetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "def check_assets_for_repository(repo_name):\n",
        "    asset_container = []\n",
        "    repo = g.get_repo(repo_name)\n",
        "    latest_release = repo.get_latest_release()\n",
        "\n",
        "    for asset in latest_release.get_assets():\n",
        "        asset_container.append(asset)\n",
        "\n",
        "    # Check if asset_container has any data\n",
        "    if asset_container:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TEST TOKEN OF PROMPTS HERE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1400\n"
          ]
        }
      ],
      "source": [
        "def get_readme_contents(repo_name):\n",
        "    common_dirs = [\n",
        "    '',  # root directory\n",
        "    '.github'\n",
        "    ]\n",
        "    \n",
        "    ini_repo = g.get_repo(repo_name)\n",
        "    found = False\n",
        "    for directory in common_dirs:\n",
        "        try:\n",
        "            readme_path = f\"{directory}/README.md\".strip('/')\n",
        "            readme_content = ini_repo.get_contents(readme_path)\n",
        "            decoded_content = readme_content.decoded_content.decode()\n",
        "            break\n",
        "        except UnknownObjectException:\n",
        "            continue\n",
        "\n",
        "    \n",
        "    if decoded_content is None:\n",
        "        decoded_content = \"README.md not found\"\n",
        "\n",
        "    return decoded_content   \n",
        "\n",
        "def get_total_token(prompt):\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "    input_tokens=encoding.encode(prompt)\n",
        "    return len(input_tokens)\n",
        "\n",
        "\n",
        " \n",
        "decoded_content = get_readme_contents(\"loseys/BlackMamba\")\n",
        "\n",
        "prompt = \"Your task is to summarize the README.md of the given GitHub repository. Here is the content: \" + decoded_content + \".\" + \" Your output should be a concise and informative paragraph that summarizes the main purpose, features, and usage of the repository.\"\n",
        "\n",
        "\n",
        "\n",
        "total_token = get_total_token(prompt)\n",
        "\n",
        "print(total_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PROMPT TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HAHAHA: Command and Control\n",
            "HAHAHAH: Quasar is a free, open-source remote administration tool for Windows that facilitates user support, administrative tasks, and employee monitoring with features like remote desktop, keylogging, file management, encrypted communication, and task management, making it a comprehensive solution for remote administration.\n"
          ]
        }
      ],
      "source": [
        "def get_readme_contents(repo_name):\n",
        "    common_dirs = [\n",
        "    '',  # root directory\n",
        "    '.github'\n",
        "    ]\n",
        "\n",
        "    readme_files = ['README.md', 'Readme.md', 'README.rst', 'Readme.rst']\n",
        "    found = False\n",
        "    for directory in common_dirs:\n",
        "        for readme_file in readme_files:\n",
        "            try:\n",
        "                readme_path = f\"{directory}/{readme_file}\".strip('/')\n",
        "                readme_content = repo_name.get_contents(readme_path)\n",
        "                return readme_content.decoded_content.decode()\n",
        "            except UnknownObjectException:\n",
        "                continue\n",
        "\n",
        "    return \"README.md not found\"   \n",
        "\n",
        "def get_response_test(prompt):\n",
        " try:\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    )\n",
        "    response_text = response.choices[0].message.content\n",
        "    return response_text\n",
        " except Exception as e:\n",
        "    print(f\"General Error: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "ini_repo = g.get_repo(\"quasar/Quasar\")\n",
        "\n",
        "content = get_readme_contents(ini_repo)\n",
        "\n",
        "prompt = f\"\"\"Your task is to summarize the README of the given GitHub repository content, and give the appropriate TTP usage of the tool. Here is the content:\" {content} \"Your response for the readme should be a concise and informative paragraph that summarizes the main purpose, features, and usage of the repository. Your response for the TTP should be one-line. The TTP are any of this:(Reconnaissance, Resource Development, Initial Access, Execution, Persistence, Privilege Escalation, Defense Evasion, Credential Access, Discovery, Lateral Movement, Collection, Command and Control, Exfiltration, Impact. Your output should also be in a 2d string array format like this \n",
        "[\n",
        "    [\"TTP\", \"Your response here\"],\n",
        "    [\"Usage\", \"Your response here\"]\n",
        "]\n",
        "\"\"\"\n",
        "response = get_response(prompt)\n",
        "\n",
        "cleaned_response = response.replace(\"```plaintext\\n\", \"\").replace(\"\\n```\", \"\")\n",
        "\n",
        "# Parse the cleaned JSON response\n",
        "data_cleaned = json.loads(cleaned_response)\n",
        "\n",
        "# Extract TTP and Usage\n",
        "ttp = data_cleaned[0][1]\n",
        "usage = data_cleaned[1][1]\n",
        "\n",
        "# Create a dictionary to hold the extracted information\n",
        "result = {\n",
        "    \"TTP\": ttp,\n",
        "    \"Usage\": usage\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a JSON string\n",
        "result_json = json.dumps(result, indent=4)\n",
        "\n",
        "# Print or save the JSON string\n",
        "#print(result_json)\n",
        "\n",
        "json_data = (f'''{result_json}''')\n",
        "\n",
        "data = json.loads(json_data)\n",
        "\n",
        "# Extract the content from the \"TTP\" and \"Usage\" fields\n",
        "ttp = data.get(\"TTP\")\n",
        "usage = data.get(\"Usage\")\n",
        "\n",
        "# Print the extracted content\n",
        "print(\"HAHAHA:\", ttp)\n",
        "print(\"HAHAHAH:\", usage)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dummy function\n",
        "\n",
        "def extract_data(response, key):\n",
        "    # Clean the response\n",
        "    cleaned_response = response.replace(\"```plaintext\\n\", \"\").replace(\"\\n```\", \"\")\n",
        "    \n",
        "    # Parse the cleaned JSON response\n",
        "    data = json.loads(cleaned_response)\n",
        "    \n",
        "    # Extract TTP and Usage\n",
        "    ttp = data[0][1]\n",
        "    usage = data[1][1]\n",
        "    \n",
        "    # Create a dictionary to hold the extracted information\n",
        "    result = {\n",
        "        \"TTP\": ttp,\n",
        "        \"Usage\": usage\n",
        "    }\n",
        "    \n",
        "    # Return the requested data\n",
        "    return result.get(key)\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TEST README CONTENT HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_readme_contents(repo_name):\n",
        "    common_dirs = [\n",
        "    '',  # root directory\n",
        "    '.github'\n",
        "    ]\n",
        "\n",
        "    readme_files = ['README.md', 'Readme.md', 'README.rst', 'Readme.rst']\n",
        "    found = False\n",
        "    for directory in common_dirs:\n",
        "        for readme_file in readme_files:\n",
        "            try:\n",
        "                readme_path = f\"{directory}/{readme_file}\".strip('/')\n",
        "                readme_content = repo_name.get_contents(readme_path)\n",
        "                return readme_content.decoded_content.decode()\n",
        "            except UnknownObjectException:\n",
        "                continue\n",
        "\n",
        "    return \"README.md not found\"    \n",
        "\n",
        "\n",
        "ini_repo = g.get_repo(\"quasar/Quasar\")\n",
        "\n",
        "content = get_readme_contents(ini_repo)\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPECIFIC CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Brutal\n",
            "\n",
            "Brutal is a toolkit to quickly create various payload,powershell attack , virus attack and launch listener for a Human Interface Device\n",
            "\n",
            "Version Version Stage Build\n",
            "\n",
            "\n",
            "\n",
            "Donate\n",
            "If this project very help you to penetration testing and u want support me , you can give me a cup of coffee :)\n",
            "Donation\n",
            "Screenshoot\n",
            "  \n",
            "\n",
            "Video\n",
            "Check this video https://www.youtube.com/watch?v=WaqY-pQpuV0\n",
            "\n",
            "Do you want like a mr robot hacking scene when Angela moss plug usb into computer for get credential information ? you can choose payload in brutal ( optional 3 or 4 )\n",
            "\n",
            "The Goal\n",
            "Generate various payload and powershell attack without coding\n",
            "\n",
            "To help breaking computer very fast and agile :p\n",
            "\n",
            "The Payloads Compatibility > target Windows machines only\n",
            "\n",
            "Requirements\n",
            "Arduino Software ( I used v1.6.7 )\n",
            "\n",
            "TeensyDuino\n",
            "\n",
            "Linux udev rules\n",
            "\n",
            "How install all requirements ? Visit This Wiki\n",
            "\n",
            "Supported Hardware\n",
            "The following hardware has been tested and is known to work.\n",
            "\n",
            "Teensy 3.x\n",
            "\n",
            "Usb Cable\n",
            "\n",
            "ðŸ“œ Changelog\n",
            "Be sure to check out the [Changelog] and Read CHANGELOG.md\n",
            "\n",
            "Getting Started\n",
            "Copy and paste the PaensyLib folder inside your Arduino\\libraries\n",
            "git clone https://github.com/Screetsec/Brutal.git\n",
            "cd Brutal\n",
            "chmod +x Brutal.sh \n",
            "sudo ./Brutal.sh or sudo su ./Brutal.sh \n",
            "BUG ?\n",
            "Submit new issue\n",
            "Contact me\n",
            "Hey sup ? do you want ask about all my tools ? you can join me in telegram.me/offscreetsec\n",
            "Donations\n",
            "Donation: Send to bitcoin\n",
            "\n",
            "Addres Bitcoin : 1NuNTXo7Aato7XguFkvwYnTAFV2immXmjS\n",
            "\n",
            "\n",
            "\n",
            ":octocat: Credits\n",
            "Thanks to allah and Screetsec [ Edo -maland- ]\n",
            "Dracos Linux from Scratch Indonesia ( Awesome Penetration os ), you can see in http://dracos-linux.org/\n",
            "Offensive Security for the awesome OS ( http://www.offensive-security.com/ )\n",
            "http://www.kali.org/\n",
            "Jack Wilder admin in http://www.linuxsec.org\n",
            "And another open sources tool in github\n",
            "Uptodate new tools hacking visit http://www.kitploit.com\n",
            "Disclaimer\n",
            "Note: modifications, changes, or alterations to this sourcecode is acceptable, however,any public releases utilizing this code must be approved by writen this tool ( Edo -m- ).\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def remove_content_specific(response):\n",
        "    start_index = response.find(\"# Brutal\")\n",
        "\n",
        "    if start_index != -1:\n",
        "        response = response[start_index:]\n",
        "    print(response)\n",
        "\n",
        "response_generated = \"\"\"\n",
        "# Now Teensy can be use for\n",
        "Teensy like a rubber ducky , why im choose teensy ? because the price very cheap for me . tâ€™s extremely useful for executing scripts on a target machine without the need for human-to-keyboard interaction ( HID -ATTACK ) .When you insert the device, it will be detected as a keyboard, and using the microprocessor and onboard flash memory storage, you can send a very fast set of keystrokes to the targetâ€™s machine and completely compromise it, regardless of autorun. Iâ€™ve used it in my security testing to run recon or enumeration scripts, execute reverse shells, exploit local DLL hijack/privilege escalation vulnerabilities, and get all password . \n",
        "Now im develop new tools the name is  Brutal \n",
        "\n",
        "# Brutal\n",
        "\n",
        "Brutal is a toolkit to quickly create various payload,powershell attack , virus attack and launch listener for a Human Interface Device\n",
        "\n",
        "Version Version Stage Build\n",
        "\n",
        "\n",
        "\n",
        "Donate\n",
        "If this project very help you to penetration testing and u want support me , you can give me a cup of coffee :)\n",
        "Donation\n",
        "Screenshoot\n",
        "  \n",
        "\n",
        "Video\n",
        "Check this video https://www.youtube.com/watch?v=WaqY-pQpuV0\n",
        "\n",
        "Do you want like a mr robot hacking scene when Angela moss plug usb into computer for get credential information ? you can choose payload in brutal ( optional 3 or 4 )\n",
        "\n",
        "The Goal\n",
        "Generate various payload and powershell attack without coding\n",
        "\n",
        "To help breaking computer very fast and agile :p\n",
        "\n",
        "The Payloads Compatibility > target Windows machines only\n",
        "\n",
        "Requirements\n",
        "Arduino Software ( I used v1.6.7 )\n",
        "\n",
        "TeensyDuino\n",
        "\n",
        "Linux udev rules\n",
        "\n",
        "How install all requirements ? Visit This Wiki\n",
        "\n",
        "Supported Hardware\n",
        "The following hardware has been tested and is known to work.\n",
        "\n",
        "Teensy 3.x\n",
        "\n",
        "Usb Cable\n",
        "\n",
        "ðŸ“œ Changelog\n",
        "Be sure to check out the [Changelog] and Read CHANGELOG.md\n",
        "\n",
        "Getting Started\n",
        "Copy and paste the PaensyLib folder inside your Arduino\\libraries\n",
        "git clone https://github.com/Screetsec/Brutal.git\n",
        "cd Brutal\n",
        "chmod +x Brutal.sh \n",
        "sudo ./Brutal.sh or sudo su ./Brutal.sh \n",
        "BUG ?\n",
        "Submit new issue\n",
        "Contact me\n",
        "Hey sup ? do you want ask about all my tools ? you can join me in telegram.me/offscreetsec\n",
        "Donations\n",
        "Donation: Send to bitcoin\n",
        "\n",
        "Addres Bitcoin : 1NuNTXo7Aato7XguFkvwYnTAFV2immXmjS\n",
        "\n",
        "\n",
        "\n",
        ":octocat: Credits\n",
        "Thanks to allah and Screetsec [ Edo -maland- ]\n",
        "Dracos Linux from Scratch Indonesia ( Awesome Penetration os ), you can see in http://dracos-linux.org/\n",
        "Offensive Security for the awesome OS ( http://www.offensive-security.com/ )\n",
        "http://www.kali.org/\n",
        "Jack Wilder admin in http://www.linuxsec.org\n",
        "And another open sources tool in github\n",
        "Uptodate new tools hacking visit http://www.kitploit.com\n",
        "Disclaimer\n",
        "Note: modifications, changes, or alterations to this sourcecode is acceptable, however,any public releases utilizing this code must be approved by writen this tool ( Edo -m- ).\n",
        "\"\"\"\n",
        "\n",
        "removed_content= remove_content_specific(response_generated)\n",
        "\n",
        "print(removed_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3iDFnWq2nr8"
      },
      "source": [
        "# Convert JSON to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmSLjRGB1xqH",
        "outputId": "2e36dfb1-c113-40cf-eb8a-18f3d0d04c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been successfully converted from github_repositories_archive.json to github_repositories_archive.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def json_to_csv(json_file_path, csv_file_path):\n",
        "    # Read JSON data\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    # Check if data is a list of dictionaries\n",
        "    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n",
        "        raise ValueError(\"JSON file must contain an array of objects\")\n",
        "\n",
        "    # Extract column names from the first item\n",
        "    col_names = data[0].keys()\n",
        "\n",
        "    # Write to CSV file\n",
        "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=col_names)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "  json_file_name = input(\"Please enter the JSON file name (with extension): \")\n",
        "\n",
        "  current_directory = os.getcwd()\n",
        "  csv_file_name = os.path.splitext(json_file_name)[0] + '.csv'\n",
        "  csv_file_path = os.path.join(current_directory, csv_file_name)\n",
        "\n",
        "  try:\n",
        "        json_to_csv(json_file_name, csv_file_path)\n",
        "        print(f\"Data has been successfully converted from {json_file_name} to {csv_file_name}\")\n",
        "  except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
